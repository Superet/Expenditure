#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\date{January, 2015}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Expenditure Reallocation Notes
\end_layout

\begin_layout Section
Model Overview
\end_layout

\begin_layout Standard
We assume that consumers make decisions according to the following timing:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\tilde{V}(I,y,\nu)= & \max_{Q_{t},c_{t}}\sum_{t=1}^{\infty}\beta^{t-1}\left\{ E_{\epsilon}\left[\max_{e}U^{P}(e,\mathbb{\epsilon};Q_{t})\right]+u^{c}(c_{t})-C(I_{t+1})+\nu\right\} \\
s.t. & \sum_{r=1}^{R}e_{r}+e_{R+1}\leq y_{t},\quad\sum_{r=1}^{R}\frac{e_{rt}}{p_{rt}}+e_{R+2}\leq Q_{t},\quad0\leq c_{t}\leq I_{t}+Q_{t}
\end{align*}

\end_inset


\end_layout

\begin_layout Paragraph
Simplified model 
\end_layout

\begin_layout Standard
The dynamic allocation problem is hard to solve, but we are able to simplify
 the problem thanks to the separability assumption.
 We make two observations.
 First, inventory transition only depends on the quantity of the basket.
 Therefore, conditional on a basket type, allocation decisions do not affect
 the state transition.
 Second, consumption decisions do not depend on the how the expenditure
 is allocated, but only the current inventory plus the quantity purchased.
 The two features allow us to separately model the allocation decisions
 conditional on a basket type.
 Once conditional on a basket type, we do not need to solve the dynamic
 programming for 
\begin_inset Formula $\{e_{1t},\ldots,e_{St}\}$
\end_inset

, and thus reduce the allocation problem into a static model.
 Then we know the contingent preference towards retailers√ï attributes.
 We can compute the expected purchase utility conditional on a basket type
 and budget.
 Finally, we substitute the purchase utility back in the dynamic model,
 and solve for the decision path of basket type and consumption.
 Specifically, our dynamic model is simplified in three steps:
\end_layout

\begin_layout Standard
Step 1: we model expenditure allocation decisions conditional a basket type.
\end_layout

\begin_layout Standard
Step 2: we compute 
\begin_inset Quotes eld
\end_inset

inclusive value
\begin_inset Quotes erd
\end_inset

 conditional on a basket type and expenditure:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{alignat*}{1}
\omega(y,Q)= & E_{\epsilon}\left[\max_{e}U^{P}(e,\epsilon;Q_{t})\right]\\
s.t. & \sum_{r=1}^{R}e_{r}+e_{R+1}\leq y_{t},\quad\sum_{r=1}^{R}\frac{e_{rt}}{p_{rt}}+e_{R+2}\leq Q_{t}
\end{alignat*}

\end_inset


\end_layout

\begin_layout Standard
Step 3: we solve dynamic decisions of basket type and consumption.
 It can be shown that the original Bellman equation is equivalent to
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\tilde{V}(I,y,\nu)= & \max_{Q,c}\left\{ \omega(y,Q)+u^{C}(c)-C(I')+\nu+\beta E_{I',y'}\tilde{V}(I',y',\nu')\right\} \\
s.t. & 0\leq c\leq I+Q
\end{align*}

\end_inset


\end_layout

\begin_layout Paragraph
Model specification
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
U^{p}(e_{1},\dots,e_{R},e_{R+1},e_{R+2})= & \sum_{r=1}^{R}\gamma_{r}\psi_{r}\log\left(\frac{e_{r}}{p_{r}}+1\right)+\psi_{R+1}\log(e_{R+1})+\psi_{R+2}\log(e_{R+2}+c_{R+2})\\
s.t. & \sum_{r=1}^{R}e_{r}+e_{R+1}\leq y_{t},\quad\sum_{r=1}^{R}\frac{e_{rt}}{p_{rt}}+e_{R+2}\leq Q_{t}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
And the functions in dynamic functions are 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
u^{C}(c)= & \lambda\log(c+1)\\
C(I)= & \tau_{1}I
\end{align*}

\end_inset


\end_layout

\begin_layout Paragraph
Identification discussion 
\end_layout

\begin_layout Section
Model detail
\end_layout

\begin_layout Subsection
Multiple discrete-continuous model with quantity constraint and outside
 option
\end_layout

\begin_layout Standard
Lagrangian equaiton of the constrained optimization: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
L= & \sum_{r=1}^{R}\gamma_{r}\psi_{r}\log\left(\frac{e_{rt}}{p_{rt}\gamma_{rt}}+1\right)+\psi_{R+1}\log(e_{R+1})+\psi_{R+2}\log(e_{R+2}+c_{R+2})\\
 & +\mu(y-\sum_{r=1}^{R}e_{r}-e_{R+1})+\lambda(Q-\sum_{r=1}^{R}\frac{e_{rt}}{p_{rt}}-e_{R+2})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The KKT condition is 
\begin_inset Formula $\frac{\partial L}{\partial e_{r}}*e_{r}^{*}\leq0$
\end_inset

 and it implies that 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{cases}
\frac{\partial L}{\partial e_{rt}}-\mu-\frac{\lambda}{p_{rt}}<0, & \text{and }e_{rt}=0\\
\frac{\partial L}{\partial e_{rt}}-\mu-\frac{\lambda}{p_{rt}}=0, & \text{and }e_{rt}>0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
which is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{cases}
\psi_{r}\frac{1}{\frac{e_{rt}}{\gamma_{r}}+p_{rt}}-\frac{\psi_{R+1}}{e_{R+1}}-\frac{\psi_{R+2}}{p_{rt}(e_{R+2}+c_{R+2})}<0, & \text{and }e_{rt}=0\\
\psi_{r}\frac{1}{\frac{e_{rt}}{\gamma_{r}}+p_{rt}}-\frac{\psi_{R+1}}{e_{R+1}}-\frac{\psi_{R+2}}{p_{rt}(e_{R+2}+c_{R+2})}=0, & \text{and }e_{rt}>0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
Taking the logarithm of the equations above, we have 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{cases}
\epsilon_{rt}<V_{rt}, & \text{and }e_{rt}=0\\
\epsilon_{rt}=V_{rt}, & \text{and }e_{rt}>0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
V_{rt}=\log\left(\frac{e_{rt}}{\gamma_{r}}+p_{rt}\right)-X_{rt}\beta+\log\left(\frac{\psi_{R+1}}{e_{R+1}}+\frac{\psi_{R+2}}{p_{r}(e_{R+2}+c_{R+2})}\right)
\]

\end_inset


\end_layout

\begin_layout Subsection
Jacobian matrix
\end_layout

\begin_layout Standard
We parameter 
\begin_inset Formula $\psi_{r}=\exp(X_{r}\alpha)$
\end_inset

, 
\begin_inset Formula $\psi_{R+2}=\exp(\alpha_{0})$
\end_inset

.
\end_layout

\begin_layout Standard
We drop household and time subscript here.
 We denote the set of retailers where a household have positive spending
 is 
\begin_inset Formula $\mathcal{R}_{p}$
\end_inset

 with size 
\begin_inset Formula $n_{p}$
\end_inset

, and the set of retailers where a household does not visit is 
\begin_inset Formula $\mathcal{R}_{0}$
\end_inset

 with size of n0.
 The probability of observing an expenditure vector (e1,...,eR) is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(e_{1},\dots,e_{R})=|J|\prod_{r\in\mathcal{R}_{p}}\phi(V_{r};0,\sigma)\prod_{r\in\mathcal{R}_{0}}\Phi(V_{r};0,\sigma)
\]

\end_inset


\end_layout

\begin_layout Standard
If the error terms are assumed to follow extreme value distribtion, then
 the probability is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(e_{1},\dots,e_{R})=|J|\prod_{r\in\mathcal{R}_{p}}\frac{\exp(-V_{r}/\sigma)}{\sigma}\exp(-e^{-\frac{V_{r}}{\sigma}})\prod_{r\in\mathcal{R}_{0}}\exp(-e^{-\frac{V_{r}}{\sigma}})
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $J$
\end_inset

 is 
\begin_inset Formula $n_{p}\times n_{p}$
\end_inset

 Jacobian matrix.
 Recall that 
\begin_inset Formula $V_{r}=\log\left(\frac{e_{r}}{\gamma_{r}}+p_{r}\right)-X_{r}\alpha+\log\left(\frac{\psi_{R+1}}{e_{R+1}}+\frac{\psi_{R+2}}{p_{r}(e_{R+2}+c_{R+2})}\right)$
\end_inset

.
\end_layout

\begin_layout Standard
Let 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\lambda_{1}= & \frac{\psi_{R+1}}{e_{R+1}^{2}}\\
\lambda_{2}= & \frac{\psi_{R+2}}{(e_{R+2}+c_{R+2})^{2}}\\
\Lambda_{i}= & \frac{\psi_{R+1}}{e_{R+1}}+\frac{\psi_{R+2}}{p_{i}(e_{R+2}+c_{R+2})}\\
d_{i}= & e_{i}+\gamma_{i}p_{i}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can write the element of the Jacobian matrix as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J_{ij}=\frac{\partial V_{i}}{\partial e_{j}}=\begin{cases}
\frac{\lambda_{1}+\frac{\lambda_{2}}{p_{i}p_{j}}}{\Lambda_{i}}+\frac{1}{d_{j}} & i=j\\
\frac{\lambda_{1}+\frac{\lambda_{2}}{p_{i}p_{j}}}{\Lambda_{i}} & i\neq j
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
We use matrix determinant lemma to compute its determinant.
 Denote 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
A= & \left(\begin{array}{ccc}
\frac{1}{d_{1}}\\
 & \ddots\\
 &  & \frac{1}{d_{np}}
\end{array}\right)\\
U=\left(\begin{array}{cc}
\frac{1}{\Lambda_{1}} & \frac{1}{\Lambda_{1}p_{1}}\\
\vdots & \vdots\\
\frac{1}{\Lambda_{np}} & \frac{1}{\Lambda_{np}p_{np}}
\end{array}\right), & \quad V^{T}=\left(\begin{array}{ccc}
\lambda_{1} & \dots & \lambda_{1}\\
\frac{\lambda_{2}}{p_{1}} & \dots & \frac{\lambda_{2}}{p_{np}}
\end{array}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We know that 
\begin_inset Formula $|J|=|A+UV^{T}|=det(I+V^{T}A^{-1}U)det(A)$
\end_inset

.
 
\begin_inset Formula $|A|=\prod_{i}^{np}\frac{1}{d_{i}}$
\end_inset

.
 Now let's compute the 
\begin_inset Formula $V^{T}A^{-1}U$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
V^{T}A^{-1}U= & \left(\begin{array}{ccc}
\lambda_{1} & \dots & \lambda_{1}\\
\frac{\lambda_{2}}{p_{1}} & \dots & \frac{\lambda_{2}}{p_{np}}
\end{array}\right)\left(\begin{array}{ccc}
d_{1}\\
 & \ddots\\
 &  & d_{np}
\end{array}\right)\left(\begin{array}{cc}
\frac{1}{\Lambda_{1}} & \frac{1}{\Lambda_{1}p_{1}}\\
\vdots & \vdots\\
\frac{1}{\Lambda_{np}} & \frac{1}{\Lambda_{np}p_{np}}
\end{array}\right)\\
= & \left(\begin{array}{ccc}
\lambda_{1}d_{1} & \dots & \lambda_{1}d_{np}\\
\frac{\lambda_{2}d_{1}}{p_{1}} & \dots & \frac{\lambda_{2}d_{np}}{p_{np}}
\end{array}\right)\left(\begin{array}{cc}
\frac{1}{\Lambda_{1}} & \frac{1}{\Lambda_{1}p_{1}}\\
\vdots & \vdots\\
\frac{1}{\Lambda_{np}} & \frac{1}{\Lambda_{np}p_{np}}
\end{array}\right)\\
= & \left(\begin{array}{cc}
\lambda_{1}\sum_{i}\frac{d_{i}}{\Lambda_{i}} & \lambda_{1}\sum_{i}\frac{d_{i}}{\Lambda_{i}p_{i}}\\
\lambda_{2}\sum_{i}\frac{d_{i}}{p_{i}\Lambda_{i}} & \lambda_{2}\sum_{i}\frac{d_{i}}{\Lambda_{i}p_{i}^{2}}
\end{array}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Hence, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
|J|=\underbrace{\left[\left(1+\lambda_{1}\sum_{i}\frac{d_{i}}{\Lambda_{i}}\right)\left(1+\lambda_{2}\sum_{i}\frac{d_{i}}{\Lambda_{i}p_{i}^{2}}\right)-\lambda_{1}\lambda_{2}\left(\sum_{i}\frac{d_{i}}{p_{i}\Lambda_{i}}\right)^{2}\right]}_{J_{1}}\prod_{i}^{np}\frac{1}{d_{i}}
\]

\end_inset


\end_layout

\begin_layout Standard
Taking the logritham of the probability function, we have 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
l= & \log\left[\left(1+\lambda_{1}\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{\Lambda_{i}}\right)\left(1+\lambda_{2}\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{\Lambda_{i}p_{i}^{2}}\right)-\lambda_{1}\lambda_{2}\left(\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{p_{i}\Lambda_{i}}\right)^{2}\right]-\sum_{i\in\mathcal{R}_{p}}\log(d_{i})\\
 & +\sum_{r\in\mathcal{R}_{p}}\left[-\log(\sqrt{2\pi}\sigma)-\frac{V_{r}^{2}}{2\sigma^{2}}\right]+\sum_{r\in\mathcal{R}_{0}}\log\left(\Psi(V_{r})\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
If the error terms follow extreme value distribution, then the log likelihood
 is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
l=\log(J_{1})-\sum_{i\in\mathcal{R}_{p}}\log(d_{i})-\sum_{i=2}^{R}e^{-\frac{V_{i}}{\sigma}}-\sum_{i\in\mathcal{R}_{p}}\left(\frac{V_{i}}{\sigma}+\log(\sigma)\right)
\]

\end_inset


\end_layout

\begin_layout Subparagraph
Gradiant 
\end_layout

\begin_layout Standard
First the gradient w.r.t.
 
\begin_inset Formula $\alpha_{j}$
\end_inset

 is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial l}{\partial\alpha_{j}}=\sum_{r\in\mathcal{R}_{p}}\frac{V_{r}}{\sigma^{2}}X_{rj}-\sum_{r\in\mathcal{R}_{0}}\frac{\phi(V_{r})}{\Phi(V_{r})}X_{rj}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial l}{\partial\alpha_{0r}}=-I(r\in\mathcal{R}_{p})\frac{V_{r}}{\sigma^{2}}+I(r\in\mathcal{R}_{0})\frac{\phi(V_{r})}{\Phi(V_{r})}
\]

\end_inset


\end_layout

\begin_layout Standard
If the error terms follow extreme distribution, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial l}{\partial\alpha_{j}}=\frac{1}{\sigma}\sum_{r=1}^{R}e^{-\frac{V_{r}}{\sigma}}X_{rj}-\frac{1}{\sigma}\sum_{r\in\mathcal{R}_{p}}X_{rj}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial l}{\partial\alpha_{0r}}=\frac{1}{\sigma}e^{-\frac{V_{r}}{\sigma}}-\frac{I(r\in\mathcal{R}_{p})}{\sigma}
\]

\end_inset


\end_layout

\begin_layout Standard
Then the gradient w.r.t.
 
\begin_inset Formula $\gamma_{r}$
\end_inset

 dependents on whether 
\begin_inset Formula $r\in\mathcal{R}_{p}$
\end_inset

, if so, then
\begin_inset Formula 
\begin{align*}
\frac{\partial l}{\partial\gamma_{r}}= & \frac{1}{J_{1}}\left[\left(1+\lambda_{1}\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{\Lambda_{i}}\right)\lambda_{2}\frac{p_{r}}{\Lambda_{r}p_{r}^{2}}+\left(1+\lambda_{2}\sum_{i}\frac{d_{i}}{\Lambda_{i}p_{i}^{2}}\right)\lambda_{1}\frac{p_{r}}{\Lambda_{r}}-2\lambda_{1}\lambda_{2}\left(\sum_{i}\frac{d_{i}}{p_{i}\Lambda_{i}}\right)\frac{p_{r}}{p_{r}\Lambda_{r}}\right]-\frac{p_{r}}{d_{r}}-\frac{V_{r}^{2}}{\sigma^{2}}\frac{e_{r}/\gamma_{r}}{d_{r}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
In the equation above, note that 
\begin_inset Formula $\frac{\partial d_{r}}{\partial\gamma_{r}}=p_{r}$
\end_inset

 and 
\begin_inset Formula $\frac{\partial V_{r}}{\partial\gamma_{r}}=-\frac{e_{r}}{\gamma_{r}d_{r}}$
\end_inset

.
 In sum 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial l}{\partial\gamma_{r}}=\begin{cases}
\frac{1}{J_{1}}\left[\left(1+\lambda_{1}\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{\Lambda_{i}}\right)\lambda_{2}\frac{1}{\Lambda_{r}p_{r}}+\left(1+\lambda_{2}\sum_{i}\frac{d_{i}}{\Lambda_{i}p_{i}^{2}}\right)\lambda_{1}\frac{p_{r}}{\Lambda_{r}}-2\lambda_{1}\lambda_{2}\left(\sum_{i}\frac{d_{i}}{p_{i}\Lambda_{i}}\right)\frac{1}{\Lambda_{r}}\right]-\frac{p_{r}}{d_{r}}-\frac{V_{r}}{\sigma^{2}}\frac{e_{r}}{\gamma_{r}d_{r}}, & r\in\mathcal{R}_{p}\\
-\frac{\phi(V_{r})}{\Phi(V_{r})}\frac{e_{r}}{\gamma_{r}d_{r}}, & r\in\mathcal{R}_{0}
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
Write the gradient w.r.t.
 
\begin_inset Formula $\alpha_{q}$
\end_inset

, we see 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial l}{\partial\alpha_{q}}= & \frac{1}{J_{1}}\frac{\partial J_{1}}{\partial\alpha_{q}}-\sum_{r\in\mathcal{R}_{p}}\frac{V_{r}}{\sigma^{2}}\frac{\partial V_{r}}{\partial\alpha_{q}}+\sum_{r\in\mathcal{R}_{0}}\frac{\phi(V_{r})}{\Phi(V_{r})}\frac{\partial V_{r}}{\partial\alpha_{q}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $J_{1}$
\end_inset

 is the components within the first 
\begin_inset Formula $\log$
\end_inset

 parenthesis.
 To find the gradient of 
\begin_inset Formula $\frac{\partial l}{\partial\alpha_{q}}$
\end_inset

, we notice that 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial\lambda_{2}}{\partial\alpha_{q}}= & \frac{1}{(e_{R+2}+c_{R+2})^{2}}\frac{\partial\psi_{R+2}}{\partial\alpha_{q}}=\lambda_{2}\\
\frac{\partial\Lambda_{i}}{\partial\alpha_{q}}= & \frac{\psi_{R+2}}{p_{i}(e_{R+2}+c_{R+2})}\\
\frac{\partial V_{i}}{\partial\alpha_{q}}= & \frac{\frac{\psi_{R+2}}{p_{i}(e_{R+2}+c_{R+2})}}{\Lambda_{i}}=\delta_{i2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Then 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial J_{1}}{\partial\alpha_{q}}= & \left(1+\lambda_{1}\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{\Lambda_{i}}\right)\left[-\lambda_{2}\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{\Lambda_{i}^{2}p_{i}^{2}}\frac{\psi_{R+2}}{p_{i}(e_{R+2}+c_{R+2})}+\lambda_{2}\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{\Lambda_{i}p_{i}^{2}}\right]\\
 & +\left(1+\lambda_{2}\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{\Lambda_{i}p_{i}^{2}}\right)\left(-\lambda_{1}\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{\Lambda_{i}^{2}}\frac{\psi_{R+2}}{p_{i}(e_{R+2}+c_{R+2})}\right)\\
 & -\left[\lambda_{1}\lambda_{2}*2\left(\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{p_{i}\Lambda_{i}}\right)\left(-\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{p_{i}\Lambda_{i}^{2}}\frac{\psi_{R+2}}{p_{i}(e_{R+2}+c_{R+2})}\right)+\lambda_{1}\left(\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{p_{i}\Lambda_{i}}\right)^{2}\lambda_{2}\right]\\
= & \left(1+\lambda_{1}\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{\Lambda_{i}}\right)\lambda_{2}\left(\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{\Lambda_{i}p_{i}^{2}}\frac{\frac{\psi_{R+1}}{e_{R+1}}}{\Lambda_{i}}\right)\\
 & -\lambda_{1}\left(1+\lambda_{2}\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{\Lambda_{i}p_{i}^{2}}\right)\left(\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}\delta_{i2}}{\Lambda_{i}}\right)\\
 & -\lambda_{1}\lambda_{2}\left(\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{p_{i}\Lambda_{i}}\right)\left[\left(\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{p_{i}\Lambda_{i}}\right)-2\left(\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}\delta_{i2}}{p_{i}\Lambda_{i}}\right)\right]\\
= & \frac{\psi_{R+1}}{e_{R+1}}\lambda_{2}\left(1+\lambda_{1}\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{\Lambda_{i}}\right)\left(\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{\Lambda_{i}^{2}p_{i}^{2}}\right)-\lambda_{1}\left(1+\lambda_{2}\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{\Lambda_{i}p_{i}^{2}}\right)\left(\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}\delta_{i2}}{\Lambda_{i}}\right)\\
 & -\lambda_{1}\lambda_{2}\left(\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{p_{i}\Lambda_{i}}\right)\left(\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}(1-2\delta_{i2})}{p_{i}\Lambda_{i}}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Hence, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial l}{\partial\alpha_{q}}= & \frac{1}{J_{1}}\frac{\partial J_{1}}{\partial\alpha_{q}}-\sum_{r\in\mathcal{R}_{p}}\frac{V_{r}}{\sigma^{2}}\delta_{r2}+\sum_{r\in\mathcal{R}_{0}}\frac{\phi(V_{r})}{\Phi(V_{r})}\delta_{r2}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
And a similar formular for the gradient w.r.t.
 
\begin_inset Formula $\alpha_{y}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial l}{\partial\alpha_{y}}= & \frac{1}{J_{1}}\frac{\partial J_{1}}{\partial\alpha_{y}}-\sum_{r\in\mathcal{R}_{p}}\frac{V_{r}}{\sigma^{2}}\delta_{r1}+\sum_{r\in\mathcal{R}_{0}}\frac{\phi(V_{r})}{\Phi(V_{r})}\delta_{r1}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
where 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\delta_{i1}= & \frac{\frac{\psi_{R+1}}{e_{R+1}}}{\Lambda_{i}}\\
\frac{\partial J_{1}}{\partial\alpha_{y}}= & -\frac{\psi_{R+1}}{e_{R+1}}\lambda_{2}\left(1+\lambda_{1}\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{\Lambda_{i}}\right)\left(\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{\Lambda_{i}^{2}p_{i}^{2}}\right)+\lambda_{1}\left(1+\lambda_{2}\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{\Lambda_{i}p_{i}^{2}}\right)\left(\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}(1-\delta_{i1})}{\Lambda_{i}}\right)\\
 & -\lambda_{1}\lambda_{2}\left(\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}}{p_{i}\Lambda_{i}}\right)\left(\sum_{i\in\mathcal{R}_{p}}\frac{d_{i}(1-2\delta_{i1})}{p_{i}\Lambda_{i}}\right)\\
= & -\frac{\partial J_{1}}{\partial\alpha_{q}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The last equality holds because 
\begin_inset Formula $\delta_{i2}=1-\delta_{i1}$
\end_inset

, and 
\begin_inset Formula $1-2\delta_{i2}=\delta_{i1}-\delta_{i2}=-(1-2\delta_{i1})$
\end_inset

.
 Therefore 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial l}{\partial\alpha_{y}}=-\frac{1}{J_{1}}\frac{\partial J_{1}}{\partial\alpha_{q}}-\sum_{r\in\mathcal{R}_{p}}\frac{V_{r}}{\sigma^{2}}(1-\delta_{r2})+\sum_{r\in\mathcal{R}_{0}}\frac{\phi(V_{r})}{\Phi(V_{r})}(1-\delta_{r2})=-\frac{\partial J_{1}}{\partial\alpha_{q}}-\sum_{r\in\mathcal{R}_{p}}\frac{V_{r}}{\sigma^{2}}+\sum_{r\in\mathcal{R}_{0}}\frac{\phi(V_{r})}{\Phi(V_{r})}
\]

\end_inset


\end_layout

\begin_layout Subsection
Multiple discrete-continuous model without outside option or quantity constraint
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
L= & \sum_{r=1}^{R}\gamma_{r}\psi_{r}\log\left(\frac{e_{rt}}{p_{rt}\gamma_{rt}}+1\right)+\psi_{R+2}\log(e_{R+2}+c_{R+2})\\
 & +\mu(y-\sum_{r=1}^{R}e_{r})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The KKT condition is 
\begin_inset Formula $\frac{\partial L}{\partial e_{r}}*e_{r}^{*}\leq0$
\end_inset

 and it implies that 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{cases}
\frac{\partial U}{\partial e_{rt}}-\mu<0, & \text{and }e_{rt}=0\\
\frac{\partial U}{\partial e_{rt}}-\mu=0, & \text{and }e_{rt}>0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
For simplicity of notation, assume that expenditure in the first retailer
 is positive.
 Therefore, 
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $ $
\end_inset

we have 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial L}{\partial e_{1}}= & \psi_{1}\frac{1}{\frac{e_{1}}{\gamma_{1}}+p_{1}}-\mu=0
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Then we can write 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{cases}
\psi_{r}\frac{1}{\frac{e_{rt}}{\gamma_{r}}+p_{rt}}-\psi_{1}\frac{1}{\frac{e_{1}}{\gamma_{1}}+p_{1}}<0, & \text{and }e_{rt}=0\\
\psi_{r}\frac{1}{\frac{e_{rt}}{\gamma_{r}}+p_{rt}}-\psi_{1}\frac{1}{\frac{e_{1}}{\gamma_{1}}+p_{1}}=0, & \text{and }e_{rt}>0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
Taking the logarithm of the equations above, we have 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{cases}
\epsilon_{rt}<V_{rt}, & \text{and }e_{rt}=0\\
\epsilon_{rt}=V_{rt}, & \text{and }e_{rt}>0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
V_{rt}=\log\left(\frac{e_{rt}}{\gamma_{r}}+p_{rt}\right)-X_{rt}\beta+X_{1}\beta-\log\left(\frac{e_{1t}}{\gamma_{1}}+p_{1}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
The Jacobian matrix is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J_{ij}=\begin{cases}
\frac{1}{e_{1}+\gamma_{1}p_{1}} & i\neq j\\
\frac{1}{e_{1}+\gamma_{1}p_{1}}+\frac{1}{e_{i}+\gamma_{i}p_{i}} & i=j
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $d_{i}=e_{i}+\gamma_{i}p_{i}$
\end_inset

.
 It can be shown that the Jacobian matrix 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
|J|=\left(\sum_{i=1}^{np}d_{i}\right)\left(\prod_{i=1}^{np}\frac{1}{d_{i}}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Hence, the log likelihood of one observation is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
l= & \log\left[\sum_{i=1}^{np}d_{i}\right]-\sum_{i\in\mathcal{R}_{p}}\log(d_{i})\\
 & +\sum_{r\in\mathcal{R}_{p}}\left[-\log(\sqrt{2\pi}\sigma)-\frac{V_{r}^{2}}{2\sigma^{2}}\right]+\sum_{r\in\mathcal{R}_{0}}\log\left(\Psi(V_{r})\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
If the error terms follow extreme value distribution, then the log likelihood
 is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
l=\log(\sum_{i=1}^{np}d_{i})-\sum_{i\in\mathcal{R}_{p}}\log(d_{i})-\sum_{i=2}^{R}e^{-\frac{V_{i}}{\sigma}}-\sum_{i\in\mathcal{R}_{p}}\left(\frac{V_{i}}{\sigma}+\log(\sigma)\right)
\]

\end_inset


\end_layout

\begin_layout Paragraph*
Gradient
\end_layout

\begin_layout Standard
First the gradient w.r.t.
 
\begin_inset Formula $\alpha_{j}$
\end_inset

 is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial l}{\partial\alpha_{j}}=\sum_{r\in\mathcal{R}_{p}}\frac{V_{r}}{\sigma^{2}}\left(X_{1j}-X_{rj}\right)-\sum_{r\in\mathcal{R}_{0}}\frac{\phi(V_{r})}{\Phi(V_{r})}\left(X_{1j}-X_{rj}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial l}{\partial\alpha_{0r}}=-I(r\in\mathcal{R}_{p})\frac{V_{r}}{\sigma^{2}}+I(r\in\mathcal{R}_{0})\frac{\phi(V_{r})}{\Phi(V_{r})}
\]

\end_inset


\end_layout

\begin_layout Standard
If the error terms follow extreme distribution, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial l}{\partial\alpha_{j}}=\frac{1}{\sigma}\sum_{r=1}^{R}e^{-\frac{V_{r}}{\sigma}}\left(X_{1j}-X_{rj}\right)-\frac{1}{\sigma}\sum_{r\in\mathcal{R}_{p}}\left(X_{1j}-X_{rj}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial l}{\partial\alpha_{0r}}=\begin{cases}
-\frac{1}{\sigma}e^{-\frac{V_{r}}{\sigma}}+\frac{I(r\in\mathcal{R}_{p})}{\sigma} & r\notin1\\
\sum_{i=2}^{R}e^{-\frac{V_{i}}{\sigma}}-\sum_{i\in\mathcal{R}_{p}}\frac{1}{\sigma} & r\in1
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Subsection
Multiple discrete-continuous model with quantity constraint but not outside
 option
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
L= & \sum_{r=1}^{R}\gamma_{r}\psi_{r}\log\left(\frac{e_{rt}}{p_{rt}\gamma_{rt}}+1\right)+\psi_{R+2}\log(e_{R+2}+c_{R+2})\\
 & +\mu(y-\sum_{r=1}^{R}e_{r})+\lambda(Q-\sum_{r=1}^{R}\frac{e_{rt}}{p_{rt}}-e_{R+2})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The KKT condition is 
\begin_inset Formula $\frac{\partial L}{\partial e_{r}}*e_{r}^{*}\leq0$
\end_inset

 and it implies that 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{cases}
\frac{\partial U}{\partial e_{rt}}-\mu-\frac{\lambda}{p_{r}}<0, & \text{and }e_{rt}=0\\
\frac{\partial U}{\partial e_{rt}}-\mu-\frac{\lambda}{p_{r}}=0, & \text{and }e_{rt}>0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
For simplicity of notation, assume that expenditure in the first retailer
 is positive.
 Therefore, 
\begin_inset Formula $ $
\end_inset


\begin_inset Formula $ $
\end_inset

we have 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial L}{\partial e_{1}}= & \psi_{1}\frac{1}{\frac{e_{1}}{\gamma_{1}}+p_{1}}-\mu-\frac{\lambda}{p_{1}}=0\\
\frac{\partial L}{\partial e_{R+2}}= & \frac{\psi_{R+2}}{e_{R+2}+c_{R+2}}-\lambda=0
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
which is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{cases}
\frac{\partial L}{\partial e_{rt}}-\mu-\frac{\lambda}{p_{rt}}<0, & \text{and }e_{rt}=0\\
\frac{\partial L}{\partial e_{rt}}-\mu-\frac{\lambda}{p_{rt}}=0, & \text{and }e_{rt}>0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
which is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{cases}
\psi_{r}\frac{1}{\frac{e_{rt}}{\gamma_{r}}+p_{rt}}-\left(\frac{\psi_{1}\gamma_{1}}{e_{1}+\gamma_{1}p_{1}}-\frac{\psi_{R+2}}{p_{1}(e_{R+2}+c_{R+2})}\right)-\frac{\psi_{R+2}}{p_{rt}(e_{R+2}+c_{R+2})}<0, & \text{and }e_{rt}=0\\
\psi_{r}\frac{1}{\frac{e_{rt}}{\gamma_{r}}+p_{rt}}-\left(\frac{\psi_{1}\gamma_{1}}{e_{1}+\gamma_{1}p_{1}}-\frac{\psi_{R+2}}{p_{1}(e_{R+2}+c_{R+2})}\right)-\frac{\psi_{R+2}}{p_{rt}(e_{R+2}+c_{R+2})}=0, & \text{and }e_{rt}>0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
Taking the logarithm of the equations above, we have 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{cases}
\epsilon_{rt}<V_{rt}, & \text{and }e_{rt}=0\\
\epsilon_{rt}=V_{rt}, & \text{and }e_{rt}>0
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
V_{rt}=\log\left(\frac{e_{rt}}{\gamma_{r}}+p_{rt}\right)-X_{rt}\beta+\log\left(\frac{\psi_{1}\gamma_{1}}{e_{1}+\gamma_{1}p_{1}}+\frac{\psi_{R+2}}{(e_{R+2}+c_{R+2})}\left(\frac{1}{p_{r}}-\frac{1}{p_{1}}\right)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
Let 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\lambda_{1}= & \frac{\psi_{1}\gamma_{1}}{\left(e_{1}+p_{1}\gamma_{1}\right)^{2}}\\
\lambda_{2}= & \frac{\psi_{R+2}}{(e_{R+2}+c_{R+2})^{2}}\\
\tilde{p}_{i}= & \frac{1}{p_{i}}-\frac{1}{p_{1}}\\
\Lambda_{i}= & \psi_{1}\frac{\gamma_{1}}{e_{1}+p_{1}\gamma_{1}}+\frac{\psi_{R+2}\tilde{p}_{i}}{(e_{R+2}+c_{R+2})}\\
d_{i}= & e_{i}+\gamma_{i}p_{i}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We can write the element of the Jacobian matrix as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J_{ij}=\frac{\partial V_{i}}{\partial e_{j}}=\begin{cases}
\frac{\lambda_{1}+\tilde{p}_{i}\lambda_{2}\tilde{p}_{j}}{\Lambda_{i}}+\frac{1}{d_{j}} & i=j\\
\frac{\lambda_{1}+\tilde{p}_{i}\lambda_{2}\tilde{p}_{j}}{\Lambda_{i}} & i\neq j
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Standard
We use matrix determinant lemma to compute its determinant.
 Denote 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
A= & \left(\begin{array}{ccc}
\frac{1}{d_{2}}\\
 & \ddots\\
 &  & \frac{1}{d_{np}}
\end{array}\right)\\
U=\left(\begin{array}{cc}
\frac{1}{\Lambda_{2}} & \frac{\tilde{p}_{2}}{\Lambda_{2}}\\
\vdots & \vdots\\
\frac{1}{\Lambda_{np}} & \frac{\tilde{p}_{np}}{\Lambda_{np}}
\end{array}\right), & \quad V^{T}=\left(\begin{array}{ccc}
\lambda_{1} & \dots & \lambda_{1}\\
\lambda_{2}\tilde{p}_{2} & \dots & \lambda_{2}\tilde{p}_{np}
\end{array}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We know that 
\begin_inset Formula $|J|=|A+UV^{T}|=det(I+V^{T}A^{-1}U)det(A)$
\end_inset

.
 
\begin_inset Formula $|A|=\prod_{i}^{np}\frac{1}{d_{i}}$
\end_inset

.
 Now let's compute the 
\begin_inset Formula $V^{T}A^{-1}U$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
V^{T}A^{-1}U= & \left(\begin{array}{ccc}
\lambda_{1} & \dots & \lambda_{1}\\
\lambda_{2}(\frac{1}{p_{2}}-\frac{1}{p_{1}}) & \dots & \lambda_{2}(\frac{1}{p_{np}}-\frac{1}{p_{1}})
\end{array}\right)\left(\begin{array}{ccc}
d_{2}\\
 & \ddots\\
 &  & d_{np}
\end{array}\right)\left(\begin{array}{cc}
\frac{1}{\Lambda_{2}} & \frac{(\frac{1}{p_{2}}-\frac{1}{p_{1}})}{\Lambda_{2}}\\
\vdots & \vdots\\
\frac{1}{\Lambda_{np}} & \frac{(\frac{1}{p_{np}}-\frac{1}{p_{1}})}{\Lambda_{np}}
\end{array}\right)\\
= & \left(\begin{array}{ccc}
\lambda_{1}d_{2} & \dots & \lambda_{1}d_{np}\\
\lambda_{2}d_{2}(\frac{1}{p_{2}}-\frac{1}{p_{1}}) & \dots & \lambda_{2}d_{np}(\frac{1}{p_{np}}-\frac{1}{p_{1}})
\end{array}\right)\left(\begin{array}{cc}
\frac{1}{\Lambda_{2}} & \frac{(\frac{1}{p_{2}}-\frac{1}{p_{1}})}{\Lambda_{2}}\\
\vdots & \vdots\\
\frac{1}{\Lambda_{np}} & \frac{(\frac{1}{p_{np}}-\frac{1}{p_{1}})}{\Lambda_{np}}
\end{array}\right)\\
= & \left(\begin{array}{cc}
\lambda_{1}\sum_{i}\frac{d_{i}}{\Lambda_{i}} & \lambda_{1}\sum_{i}\frac{d_{i}}{\Lambda_{i}}\tilde{p}_{i}\\
\lambda_{2}\sum_{i}\frac{d_{i}}{\Lambda_{i}}\tilde{p}_{i} & \lambda_{2}\sum_{i}\frac{d_{i}}{\Lambda_{i}}\tilde{p}_{i}^{2}
\end{array}\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Hence, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
|J|=\left[\left(1+\lambda_{1}\sum_{i}\frac{d_{i}}{\Lambda_{i}}\right)\left(1+\lambda_{2}\sum_{i}\frac{d_{i}\tilde{p}_{i}^{2}}{\Lambda_{i}}\right)-\lambda_{1}\lambda_{2}\left(\sum_{i}\frac{d_{i}\tilde{p}_{i}}{\Lambda_{i}}\right)^{2}\right]\prod_{i=2}^{np}\frac{1}{d_{i}}
\]

\end_inset


\end_layout

\begin_layout Standard
We notice that 
\begin_inset Formula $\frac{1}{\lambda_{1}}=\frac{\psi_{1}\gamma_{1}}{(e_{1}+p_{1}\gamma_{1})^{2}}=\frac{d_{1}}{\Lambda_{1}}$
\end_inset

, and thus 
\begin_inset Formula $J$
\end_inset

 can be simplied as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
J_{1}=\lambda_{1}\lambda_{2}\left[\left(\sum_{i=1}^{np}\frac{d_{i}}{\Lambda_{i}}\right)\left(\frac{1}{\lambda_{2}}+\sum_{i=1}^{np}\frac{d_{i}\tilde{p}_{i}^{2}}{\Lambda_{i}}\right)-\left(\sum_{i}\frac{d_{i}\tilde{p}_{i}}{\Lambda_{i}}\right)^{2}\right]
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
J= & \lambda_{1}\lambda_{2}\left[\left(\sum_{i=1}^{np}\frac{d_{i}}{\Lambda_{i}}\right)\left(\frac{1}{\lambda_{2}}+\sum_{i=1}^{np}\frac{d_{i}\tilde{p}_{i}^{2}}{\Lambda_{i}}\right)-\left(\sum_{i}\frac{d_{i}\tilde{p}_{i}}{\Lambda_{i}}\right)^{2}\right]\prod_{i=2}^{np}\frac{1}{d_{i}}\\
= & \frac{\psi_{1}\gamma_{1}}{d_{1}}\frac{\psi_{R+2}}{(e_{R+2}+c_{R+2})^{2}}\left[\left(\sum_{i=1}^{np}\frac{d_{i}}{\Lambda_{i}}\right)\left(\frac{1}{\lambda_{2}}+\sum_{i=1}^{np}\frac{d_{i}\tilde{p}_{i}^{2}}{\Lambda_{i}}\right)-\left(\sum_{i}\frac{d_{i}\tilde{p}_{i}}{\Lambda_{i}}\right)^{2}\right]\prod_{i=1}^{np}\frac{1}{d_{i}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Taking the logritham of the probability function, we have 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
l= & \log(J_{1})-\sum_{i\in\mathcal{R}_{p}}\log(d_{i})+\sum_{r\in\mathcal{R}_{p}}\left[-\log(\sqrt{2\pi}\sigma)-\frac{V_{r}^{2}}{2\sigma^{2}}\right]+\sum_{r\in\mathcal{R}_{0}}\log\left(\Psi(V_{r})\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
If the error terms follow extreme value distribution, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
l=\log(J_{1})-\sum_{i\in\mathcal{R}_{p}}\log(d_{i})-\sum_{i=2}^{R}e^{-\frac{V_{i}}{\sigma}}-\sum_{i\in\mathcal{R}_{p}}\left(\frac{V_{i}}{\sigma}+\log(\sigma)\right)
\]

\end_inset


\end_layout

\begin_layout Subparagraph
Gradiant 
\end_layout

\begin_layout Standard
First the gradient w.r.t.
 
\begin_inset Formula $\alpha_{j}$
\end_inset

 is 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\partial l}{\partial\alpha_{j}}=\frac{1}{J_{1}}\frac{\partial J_{1}}{\partial\alpha_{j}}+\sum_{r\in\mathcal{R}_{p}}\frac{V_{r}}{\sigma^{2}}X_{rj}-\sum_{r\in\mathcal{R}_{0}}\frac{\phi(V_{r})}{\Phi(V_{r})}X_{rj}
\]

\end_inset


\end_layout

\begin_layout Standard
Since 
\begin_inset Formula $\frac{\partial\lambda_{1}}{\partial\alpha_{j}}=\lambda_{1}X_{1j}$
\end_inset

 and 
\begin_inset Formula $\frac{\partial\Lambda_{i}}{\partial\alpha_{j}}=\frac{\gamma_{1}\psi_{1}}{e_{1}+p_{1}\gamma_{1}}X_{1j}=\delta_{1}X_{1j}$
\end_inset

, then we can compute
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\frac{\partial J_{1}}{\partial\alpha_{j}}= & -\left(1+\lambda_{1}\sum_{i}\frac{d_{i}}{\Lambda_{i}}\right)\lambda_{2}\sum_{i}\frac{d_{i}\tilde{p}_{i}^{2}}{\Lambda_{i}^{2}}\delta_{1}X_{1j}+\left(1+\lambda_{2}\sum_{i}\frac{d_{i}\tilde{p}_{i}^{2}}{\Lambda_{i}}\right)\left(\lambda_{1}X_{1j}\sum_{i}\frac{d_{i}}{\Lambda_{i}}-\lambda_{1}\sum_{i}\frac{d_{i}}{\Lambda_{i}^{2}}\delta_{1}X_{1j}\right)\\
 & -\lambda_{1}X_{1j}\lambda_{2}\left(\sum_{i}\frac{d_{i}\tilde{p}_{i}}{\Lambda_{i}}\right)^{2}-\lambda_{1}\lambda_{2}2\left(\sum_{i}\frac{d_{i}\tilde{p}_{i}}{\Lambda_{i}}\right)\left(-\sum_{i}\frac{d_{i}\tilde{p}_{i}}{\Lambda_{i}^{2}}\delta_{1}X_{1j}\right)\\
= & -\lambda_{2}\delta_{1}X_{1j}\left(1+\lambda_{1}\sum_{i}\frac{d_{i}}{\Lambda_{i}}\right)\sum_{i}\frac{d_{i}\tilde{p}_{i}^{2}}{\Lambda_{i}^{2}}+\lambda_{1}X_{1j}\left(1+\lambda_{2}\sum_{i}\frac{d_{i}\tilde{p}_{i}^{2}}{\Lambda_{i}}\right)\sum_{i}\frac{d_{i}}{\Lambda_{i}}(1-\frac{\delta_{1}}{\Lambda_{i}})\\
 & -\lambda_{1}\lambda_{2}X_{1j}\left(\sum_{i}\frac{d_{i}\tilde{p}_{i}}{\Lambda_{i}}\right)\left(\sum_{i}\frac{d_{i}\tilde{p}_{i}}{\Lambda_{i}}(1-\frac{2\delta_{1}}{\Lambda_{i}})\right)
\end{align*}

\end_inset


\end_layout

\end_body
\end_document
